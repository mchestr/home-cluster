---
version: "3"

vars:
  N: '{{ .N | default "0" }}'
  NODES: '{{.NODES | default "192.168.1.200,192.168.1.201,192.168.1.202,192.168.1.203"}}'
  NODE:
    sh: 'echo ${NODE:-192.168.1.20{{ .N }}}'
  # renovate: datasource=github-releases depName=siderolabs/talos
  TALOS_VERSION: "v1.3.5"
  # renovate: datasource=github-releases depName=budimanjojo/talhelper
  TALHELPER_VERSION: "v1.6.1"
  # renovate: datasource=github-releases depName=kubernetes/kubernetes
  KUBERNETES_VERSION: "v1.26.1"

tasks:

  init:
    desc: Init talosctl and grab kubeconfig
    cmds:
      - mkdir config || true
      - sudo curl -Lo /usr/local/bin/talosctl https://github.com/siderolabs/talos/releases/download/{{.TALOS_VERSION}}/talosctl-$(uname -s | tr "[:upper:]" "[:lower:]")-$(uname -m) && sudo chmod +x /usr/local/bin/talosctl
      - curl -Lo /tmp/talhelper-{{.TALHELPER_VERSION}}.tar.gz https://github.com/budimanjojo/talhelper/releases/download/{{.TALHELPER_VERSION}}/talhelper_$(uname -s | tr "[:upper:]" "[:lower:]")_$(uname -m).tar.gz && tar -xzf /tmp/talhelper-{{.TALHELPER_VERSION}}.tar.gz -C /tmp && sudo mv /tmp/talhelper /usr/local/bin/talhelper
      - task: generate
      - talosctl --talosconfig=talos/clusterconfig/talosconfig kubeconfig --nodes 192.168.1.200 $KUBECONFIG

  generate:
    desc: Generate Talos machine configurations
    dir: talos
    cmds:
      - talhelper genconfig

  apply:
    desc: Apply Talos config to a specific node (task talos:apply N=0)
    dir: talos
    cmds:
      - talosctl apply-config --nodes {{.NODE}} --file clusterconfig/home-cluster-k8s-{{.N}}.yaml {{.CLI_ARGS}}

  upgrade:k8s:
    dir: talos
    desc: Upgrade Kubernetes to latest version
    cmds:
      - talosctl --nodes 192.168.1.200 upgrade-k8s --to {{ .KUBERNETES_VERSION }}

  upgrade:
    desc: Stage the latest updates for the node (task talos:upgrade N=0), then do the upgrade.
    dir: talos
    cmds:
      - task: generate
      - task: _wait_for_health
        vars: {TIMEOUT: 10m}
      - task: apply
        vars: {CLI_ARGS: '-m staged'}
      - talosctl upgrade --nodes {{ .NODE }} --image ghcr.io/siderolabs/installer:{{ .TALOS_VERSION }}

  upgrade:all:
    desc: Upgrade Talos OS to latest
    dir: talos
    cmds:
      # Upgrade control-plane then workers, ensure cluster is healthy before each upgrade.
      - task: upgrade
        vars: {NODE: 192.168.1.200}
      - task: upgrade
        vars: {NODE: 192.168.1.201}
      - task: upgrade
        vars: {NODE: 192.168.1.203}
      - task: upgrade
        vars: {NODE: 192.168.1.202}

  memory:
    desc: Get all nodes memory
    dir: talos
    cmds:
      - talosctl --nodes {{.NODES}} memory {{.CLI_ARGS}}

  stats:
    desc: Get all nodes stats
    dir: talos
    cmds:
      - talosctl --nodes {{.NODES}} stats {{.CLI_ARGS}}

  time:
    desc: Get all nodes time
    dir: talos
    cmds:
      - talosctl --nodes {{.NODES}} time {{.CLI_ARGS}}

  services:
    desc: Get all nodes services
    dir: talos
    cmds:
      - talosctl --nodes {{.NODES}} services {{.CLI_ARGS}}

  health:
    desc: Get cluster health
    dir: talos
    cmds:
      - talosctl --nodes {{.NODE}} health {{.CLI_ARGS}}

  version:
    desc: Get cluster talos version
    dir: talos
    cmds:
      - talosctl --nodes {{.NODES}} version {{.CLI_ARGS}}

  dashboard:
    desc: Get all nodes dashboard
    dir: talos
    cmds:
      - talosctl --nodes {{.NODE}} dashboard {{.CLI_ARGS}}

  _wait_for_health:
    desc: Wait for services in cluster to be healthy
    cmds:
      # Ensure CephCluster is healthy
      - kubectl -n rook-ceph wait --for jsonpath='{.status.ceph.health}'='HEALTH_OK' --timeout {{ .TIMEOUT | default "30s" }} cephcluster rook-ceph
      # Ensure CloudNative-PG cluster has 3 ready instances
      - kubectl wait --for jsonpath='{.status.readyInstances}'='3' --timeout {{ .TIMEOUT | default "30s" }} cluster postgres
